{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Decision Tree Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code based submitted here builds a simple binary decision trees to take categorical variables as input and output a simple Decision Tree that classifies data into 2 [binary] classes e.g. BAD vs. Good, Yes vs. No\n",
    "\n",
    "This work  will use the [LendingClub](https://www.lendingclub.com/) dataset to build a Decision Tree to discern between safe  versus Non-safe loans and predict in the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [LendingClub](https://www.lendingclub.com/) data set load & cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import csv data from local folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "loans = pd.read_csv('../lending-club-data.csv/lending-club-data.csv', \\\n",
    "                    low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loans:- (Rows, Columns) (122607, 69)\n"
     ]
    }
   ],
   "source": [
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans.drop('bad_loans', axis =1)\n",
    "print 'Loans:- (Rows, Columns)', loans.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In developing the code we use the following  4 categorical features to build and validate the Decision Tree code: \n",
    "1. Grade of the loan \n",
    "2. The length of the loan term\n",
    "3. The home ownership status: own, mortgage, rent\n",
    "4. Number of years of employment.\n",
    "In the dataset, each of these features is a categorical feature. Since we are building a binary decision tree, we will have to convert this to binary data in a subsequent section using 1-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home_ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set sampling to ensure class balance\n",
    "\n",
    "A Balanced sample set with similar number of minority and majority class group is helpful for learning decision trees. While many different methodologies are available, we just choose to sample the majority class group to equate to the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Proportions(minority/ majority class): 23.28%\n",
      "Percentage of safe loans                 : 0.5\n",
      "Percentage of risky loans                : 0.5\n",
      "Total number of loans in our new dataset : 46300\n"
     ]
    }
   ],
   "source": [
    "safe_loans_raw = loans[loans[target] == 1]\n",
    "risky_loans_raw = loans[loans[target] == -1]\n",
    "\n",
    "# Since there are less risky loans than safe loans, find the ratio of the sizes\n",
    "# and use that percentage to undersample the safe loans.\n",
    "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
    "safe_loans = safe_loans_raw.sample(len(risky_loans_raw), random_state = 1)\n",
    "risky_loans = risky_loans_raw\n",
    "loans_data = risky_loans.append(safe_loans)\n",
    "\n",
    "print \"Raw Proportions(minority/ majority class):\", \"{0:2.2f}%\".format(percentage*100)\n",
    "print \"Percentage of safe loans                 :\", len(safe_loans) / float(len(loans_data))\n",
    "print \"Percentage of risky loans                :\", len(risky_loans) / float(len(loans_data))\n",
    "print \"Total number of loans in our new dataset :\", len(loans_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical data into binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are implementing **binary decision trees**, we transform our categorical data into binary data using 1-hot encoding. \n",
    "For instance, the **home_ownership** feature represents the home ownership status of the \n",
    "loanee, which is either `own`, `mortgage` or `rent`. For example, if a data point has the \n",
    "feature \n",
    "```\n",
    "   {'home_ownership': 'RENT'}\n",
    "```\n",
    "we want to turn this into three features: \n",
    "```\n",
    " { \n",
    "   'home_ownership = OWN'      : 0, \n",
    "   'home_ownership = MORTGAGE' : 0, \n",
    "   'home_ownership = RENT'     : 1\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "loans_data = risky_loans.append(safe_loans)\n",
    "#features = loans_data.columns\n",
    "for feature in features:\n",
    "    one_hot = pd.get_dummies(loans_data[feature], prefix = feature, prefix_sep='.')\n",
    "    loans_data = loans_data.join(one_hot)\n",
    "    loans_data.drop(feature, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features in the Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'grade.A', u'grade.B', u'grade.C', u'grade.D', u'grade.E', u'grade.F',\n",
      "       u'grade.G', u'term. 36 months', u'term. 60 months',\n",
      "       u'home_ownership.MORTGAGE', u'home_ownership.OTHER',\n",
      "       u'home_ownership.OWN', u'home_ownership.RENT', u'emp_length.1 year',\n",
      "       u'emp_length.10+ years', u'emp_length.2 years', u'emp_length.3 years',\n",
      "       u'emp_length.4 years', u'emp_length.5 years', u'emp_length.6 years',\n",
      "       u'emp_length.7 years', u'emp_length.8 years', u'emp_length.9 years',\n",
      "       u'emp_length.< 1 year', u'emp_length.n/a'],\n",
      "      dtype='object')\n",
      "Number of features (after binarizing categorical variables) = 25\n"
     ]
    }
   ],
   "source": [
    "features= loans_data.columns\n",
    "features = features.drop('safe_loans')\n",
    "print features\n",
    "print \"Number of features (after binarizing categorical variables) = %s\" % len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "We split the data into a train test split with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Rows in Loans Data 46300\n",
      "# Rows in train data 37030\n",
      "# Rows in test data 9270\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(df, split = 0.8):\n",
    "    np.random.seed(seed=1234)\n",
    "    msk = np.random.rand(len(df)) < 0.8\n",
    "    train = df[msk]\n",
    "    test= df[~msk]\n",
    "    return(train, test)\n",
    "\n",
    "(train, test) = train_test_split(loans_data, 0.8)\n",
    "\n",
    "print '# Rows in Loans Data',len(loans_data)\n",
    "print '# Rows in train data',len(train)\n",
    "print '# Rows in test data',len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, binary decision trees is implemented from scratch. There are several steps involved in building a decision tree. The following are the high level steps:\n",
    "\n",
    "## 1. Function to count number of Errors while predicting majority class\n",
    "\n",
    "Prediction at an intermediate node works by predicting the **majority class** for all data points that belong to this node.\n",
    "\n",
    "The function below calculates the number of **missclassified examples** when predicting the **majority class**. This will be used to help determine which feature is the best to split on at a given node of the tree.\n",
    "\n",
    "**Note**: Keep in mind that in order to compute the number of errors for a majority classifier, we only need the label (y values) of the data points in the node. \n",
    "\n",
    "** Steps to follow **:\n",
    "* ** Step 1:** Calculate the number of safe loans and risky loans.\n",
    "* ** Step 2:** Since we are assuming majority class prediction, all the data points that are **not** in the majority class are considered **errors**.\n",
    "* ** Step 3:** Return the number of **errors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_num_errors(labels_in_node):\n",
    "    # Corner case: If labels_in_node is empty, return 0\n",
    "    if len(labels_in_node) == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Count the number of 1's (safe loans)\n",
    "    safe_loans = (labels_in_node == 1).sum()\n",
    "    # Count the number of -1's (risky loans)\n",
    "    risky_loans = (labels_in_node == -1).sum()           \n",
    "    # Return the number of errors that the majority classifier makes.\n",
    "    if safe_loans > risky_loans:\n",
    "        return(risky_loans)\n",
    "    else:\n",
    "        return(safe_loans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing error count function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n",
      "Test passed!\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "# Test case 1\n",
    "example_labels = np.array([-1, -1, 1, 1, 1])\n",
    "if intermediate_node_num_errors(example_labels) == 2:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test 1 failed... try again!'\n",
    "\n",
    "# Test case 2\n",
    "example_labels =  np.array([-1, -1, 1, 1, 1, 1, 1])\n",
    "if intermediate_node_num_errors(example_labels) == 2:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test 2 failed... try again!'\n",
    "    \n",
    "# Test case 3\n",
    "example_labels =  np.array([-1, -1, -1, -1, -1, 1, 1])\n",
    "if intermediate_node_num_errors(example_labels) == 2:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test 3 failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Function to pick best feature to split on\n",
    "\n",
    "The function **best_splitting_feature** takes 3 arguments: \n",
    "1. The data (data frame which includes all of the feature columns and label column)\n",
    "2. The features to consider for splits (a list of strings of column names to consider for splits)\n",
    "3. The name of the target/label column (string)\n",
    "\n",
    "The function will loop through the list of possible features, and consider splitting on each of them. It will calculate the classification error of each split and return the feature that had the smallest classification error when split on.\n",
    "\n",
    "**classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# Errors}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "Following are the steps: \n",
    "* **Step 1:** Loop over each feature in the feature list\n",
    "* **Step 2:** Within the loop, the data is split into two groups: one group where all of the data has feature value 0 or False (we will call this the **left** split), and one group where all of the data has feature value 1 or True (we will call this the **right** split). The **left** split corresponds with 0 and the **right** split corresponds with 1 to ensure the implementation fits with the tree building process.\n",
    "* **Step 3:** Calculate the number of misclassified examples in both groups of data and the above formula is used to compute the **classification error**.\n",
    "* **Step 4:** If the computed error is smaller than the best error found so far, store this **feature and its error**.\n",
    "\n",
    "Since we are only dealing with binary features, we do not have to consider thresholds for real-valued features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def best_splitting_feature(data, features, target):\n",
    "    \n",
    "    best_feature = None # Keep track of the best feature \n",
    "    best_error = 10     # Keep track of the best error so far \n",
    "    # Note: Since error is always <= 1, we should intialize it with something larger than 1.\n",
    "\n",
    "    # Convert to float to make sure error gets computed correctly.\n",
    "    num_data_points = float(len(data))  \n",
    "    \n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        left_split = data[data[feature] == 0]\n",
    "        \n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        ## YOUR CODE HERE\n",
    "        right_split = data[data[feature] == 1]\n",
    "            \n",
    "        # Calculate the number of misclassified examples in the left split.\n",
    "        # Remember that we implemented a function for this! (It was called intermediate_node_num_errors)\n",
    "        # YOUR CODE HERE\n",
    "        left_errors = intermediate_node_num_errors(left_split[target])           \n",
    "\n",
    "        # Calculate the number of misclassified examples in the right split.\n",
    "        ## YOUR CODE HERE\n",
    "        right_errors = intermediate_node_num_errors(right_split[target])\n",
    "            \n",
    "        # Compute the classification error of this split.\n",
    "        # Error = (# of errors (left) + # of errors (right)) / (# of data points)\n",
    "        ## YOUR CODE HERE\n",
    "        error = float(left_errors+right_errors)/ float(num_data_points)\n",
    "        #print 'feature + error', feature, error\n",
    "        # If this is the best error we have found so far, store the feature as best_feature and the error as best_error\n",
    "        ## YOUR CODE HERE\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    return best_feature # Return the best feature we found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'term. 36 months'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_splitting_feature(train, features, 'safe_loans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Function to Create Leaf and build the tree\n",
    "\n",
    "Each node in the decision tree is represented as a dictionary which contains the following keys and possible values:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'splitting_feature'  : The feature that this node splits on.\n",
    "    }\n",
    "\n",
    "\n",
    "The function below creates a leaf node given a set of target values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'is_leaf': True,\n",
    "            'prediction': None,\n",
    "            'splitting_feature' : None,\n",
    "            'left' : None,\n",
    "            'right' : None}   ## YOUR CODE HERE\n",
    "    \n",
    "    # Count the number of data points that are +1 and -1 in this node.\n",
    "    num_ones = len(target_values[target_values == +1])\n",
    "    num_minus_ones = len(target_values[target_values == -1])\n",
    "    \n",
    "    # For the leaf node, set the prediction to be the majority class.\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    if num_ones > num_minus_ones:\n",
    "        leaf['prediction'] =    1      ## YOUR CODE HERE\n",
    "    else:\n",
    "        leaf['prediction'] =   -1     ## YOUR CODE HERE\n",
    "        \n",
    "    # Return the leaf node        \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Function to build the tree in a top down manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function learns the decision tree recursively and implements 3 stopping conditions:\n",
    "\n",
    "**Stopping condition 1:** All data points in a node are from the same class.\n",
    "\n",
    "**Stopping condition 2:** No more features to split on.\n",
    "\n",
    "**Stopping condition 3:** Stopping condition based on the **max_depth** of the tree. By not letting the tree grow too deep, we will save computational effort in the learning process. \n",
    "\n",
    "**Stopping condition 4:** minimum node size:\n",
    "* **Step 1:** The function **reached_minimum_node_size** detects whether we have hit the base case, i.e., the node does not have enough data points and should be turned into a leaf. \n",
    "* **Step 2:** Return a leaf using create_leaf\n",
    "\n",
    "**Stopping condition 5: minimum error reduction:**\n",
    "This has to come after finding the best splitting feature so we can calculate the error after splitting in order to calculate the error reduction.\n",
    "* **Step 1:** Calculate the **classification error before splitting**.  \n",
    "* **Step 2:** Calculate the **classification error after splitting**. This requires calculating the number of errors in the left and right splits, and then dividing by the total number of examples.\n",
    "* **Step 3:** The function **error_reduction** detects whether  the reduction in error is less than the constant provided (`min_error_reduction`). \n",
    "* **Step 4:** Return a leaf using create_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_reduction(error_before_split, error_after_split):\n",
    "    # Return the error before the split minus the error after the split.\n",
    "    gain = (error_before_split - error_after_split)\n",
    "    return(gain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reached_minimum_node_size(data, min_node_size):\n",
    "    # Return True if the number of data points is less than or equal to the minimum node size.\n",
    "    return( True if len(data) <= min_node_size else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decision_tree_create(data, features, target, current_depth = 0, \n",
    "                         max_depth = 10, min_node_size=1, \n",
    "                         min_error_reduction=0.0):\n",
    "    \n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    \n",
    "    target_values = data[target]\n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    \n",
    "    \n",
    "    # Stopping condition 1: All nodes are of the same type.\n",
    "    if intermediate_node_num_errors(target_values) == 0:\n",
    "        print \"Stopping condition 1 reached. All data points have the same class.\"                \n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # Stopping condition 2: No more features to split on.\n",
    "    if remaining_features == []:\n",
    "        print \"Stopping condition 2 reached. No remaining features.\"                \n",
    "        return create_leaf(target_values)    \n",
    "    \n",
    "    # Early stopping condition 1: Reached max depth limit.\n",
    "    if current_depth >= max_depth:\n",
    "        print \"Stopping condition 3 reached. Reached maximum depth.\"\n",
    "        return create_leaf(target_values)\n",
    "    \n",
    "    # Early stopping condition 2: Reached the minimum node size.\n",
    "    # If the number of data points is less than or equal to the minimum size, return a leaf.\n",
    "    if  reached_minimum_node_size(data, min_node_size) == True:\n",
    "        print \"Stopping condition 4 reached. Reached minimum node size.\"\n",
    "        return  create_leaf(target_values)\n",
    "    \n",
    "    # Find the best splitting feature\n",
    "    splitting_feature = best_splitting_feature(data, features, target)\n",
    "    \n",
    "    # Split on the best feature that we found. \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    # Early stopping condition 3: Minimum error reduction\n",
    "    # Calculate the error before splitting (number of misclassified examples \n",
    "    # divided by the total number of examples)\n",
    "    error_before_split = intermediate_node_num_errors(target_values) / float(len(data))\n",
    "    #print error_before_split\n",
    "    # Calculate the error after splitting (number of misclassified examples \n",
    "    # in both groups divided by the total number of examples)\n",
    "    left_errors =    intermediate_node_num_errors(left_split[target]) \n",
    "    right_errors =   intermediate_node_num_errors(right_split[target])\n",
    "    error_after_split = (left_errors + right_errors) / float(len(data))\n",
    "    #print error_before_split\n",
    "    # If the error reduction is LESS THAN OR EQUAL TO min_error_reduction, return a leaf.\n",
    "    if error_reduction(error_before_split, error_after_split) <= min_error_reduction:\n",
    "        print \"Stopping condition 5 reached. Minimum error reduction.\",\\\n",
    "                error_reduction(error_before_split, error_after_split)\n",
    "        return create_leaf(target_values)\n",
    "      \n",
    "    remaining_features.drop(splitting_feature)\n",
    "    print \"Split on feature %s. (%s, %s)\" % (\\\n",
    "                      splitting_feature, len(left_split), len(right_split))\n",
    "    \n",
    "        # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = decision_tree_create(left_split, remaining_features, target, \n",
    "                                     current_depth + 1, max_depth, min_node_size, min_error_reduction)        \n",
    "    \n",
    "    \n",
    "    right_tree = decision_tree_create(right_split, remaining_features, target, \n",
    "                                     current_depth + 1, max_depth, min_node_size, min_error_reduction)        \n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 0 (37030 data points).\n",
      "Split on feature term. 36 months. (9261, 27769)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (9261 data points).\n",
      "Split on feature grade.A. (9146, 115)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9146 data points).\n",
      "Stopping condition 3 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (115 data points).\n",
      "Stopping condition 3 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (27769 data points).\n",
      "Split on feature grade.D. (23067, 4702)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (23067 data points).\n",
      "Stopping condition 3 reached. Reached maximum depth.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (4702 data points).\n",
      "Stopping condition 3 reached. Reached maximum depth.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "small_decision_tree = decision_tree_create(train, features, 'safe_loans', max_depth = 2, \n",
    "                                        min_node_size = 10, min_error_reduction=0.0)\n",
    "if count_nodes(small_decision_tree) == 7:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'\n",
    "    print 'Number of nodes found                :', count_nodes(small_decision_tree)\n",
    "    print 'Number of nodes that should be there : 7' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Function\n",
    "\n",
    "We make predictions from the decision tree with a simple recursive function. Below, the function classify is called, which takes in a learned tree and point x to classify. We include an option annotate that describes the prediction path when set to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # if the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print \"At leaf, predicting %s\" % tree['prediction']\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print \"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error: 0.402862543883\n"
     ]
    }
   ],
   "source": [
    "predict = []\n",
    "for i in xrange(0,len(train)):\n",
    "    predict.append(classify(small_decision_tree,train.iloc[i]))\n",
    "print 'Training Error:', float(sum(predict != train['safe_loans']))/float(len(train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Error\n",
    "The end goal is to evaluate classification error of the tree. \n",
    "**Classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# errors}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "`evaluate_classification_error` takes following input:\n",
    "1. `tree` (as described above)\n",
    "2. `data` (an SFrame)\n",
    "3. `target` (a string - the name of the target/label column)\n",
    "\n",
    "This function calculates a prediction (class label) for each row in `data` using the decision `tree` and return the classification error computed using the above formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data, target):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    predict = []\n",
    "    for i in xrange(0,len(data)):\n",
    "        predict.append(classify(tree,data.iloc[i]))\n",
    "    # Once you've made the predictions, calculate the classification error and return it\n",
    "    err = data[predict!=data['safe_loans']]\n",
    "    classification_error = float(len(err))/float(len(data))\n",
    "    return (classification_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classification Error: 0.402862543883\n",
      "Test Classification Error: 0.40021574973\n"
     ]
    }
   ],
   "source": [
    "# Train Error:\n",
    "print \"Training Classification Error:\",evaluate_classification_error(small_decision_tree, train, 'safe_loans')\n",
    "# Test Error:\n",
    "print \"Test Classification Error:\",evaluate_classification_error(small_decision_tree, test, 'safe_loans')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility to visualizse the decision tree\n",
    "\n",
    "The following is a recursive print function that prints out the decision tree with following arguments:\n",
    "* tree: Name of Decision Tree\n",
    "* max_d: Maximum Depth specified for printing\n",
    "* d    : \n",
    "\n",
    "Note: Works for the linked dictionary tree structure provided here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_decision_tree(tree, max_d = None, d = 0, pre = ''):\n",
    "    if tree['is_leaf']:\n",
    "        print(pre + '* Predict %s' % tree['prediction'])\n",
    "    elif max_d is not None and d >= max_d:\n",
    "        print(pre + '...')\n",
    "    else:\n",
    "        print(pre + '+ %s = 0' % tree['splitting_feature'])\n",
    "        print_decision_tree(tree['left'], max_d, d + 1, pre + '|   ')\n",
    "        print(pre + 'L %s = 1' % tree['splitting_feature'])\n",
    "        print_decision_tree(tree['right'], max_d, d + 1, pre + '    ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ term. 36 months = 0\n",
      "|   + grade.A = 0\n",
      "|   |   * Predict -1\n",
      "|   L grade.A = 1\n",
      "|       * Predict 1\n",
      "L term. 36 months = 1\n",
      "    + grade.D = 0\n",
      "    |   * Predict 1\n",
      "    L grade.D = 1\n",
      "        * Predict -1\n"
     ]
    }
   ],
   "source": [
    "print_decision_tree(small_decision_tree, max_d = 6, d = 0, pre = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ term. 36 months = 0\n",
      "|   ...\n",
      "L term. 36 months = 1\n",
      "    ...\n"
     ]
    }
   ],
   "source": [
    "print_decision_tree(small_decision_tree, max_d = 6, d = 5, pre = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|+ term. 36 months = 0\n",
      "||   + grade.A = 0\n",
      "||   |   * Predict -1\n",
      "||   L grade.A = 1\n",
      "||       * Predict 1\n",
      "|L term. 36 months = 1\n",
      "|    + grade.D = 0\n",
      "|    |   * Predict 1\n",
      "|    L grade.D = 1\n",
      "|        * Predict -1\n"
     ]
    }
   ],
   "source": [
    "print_decision_tree(small_decision_tree, max_d = 6, d = 4, pre = '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postscript\n",
    "\n",
    "This is a simple Decision Tree implementation to solve Binary Classification problem. The complexity of the tree built can be controlled by the depth and number of leaves, to manage overfitting while giving better accuracy. In the current tree model the performance is slightly better than a random split!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
